<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "4d57fad773cbeb69c5dd62e65c34200d",
  "translation_date": "2025-10-18T00:01:55+00:00",
  "source_file": "03-using-generative-ai-responsibly/README.md",
  "language_code": "ko"
}
-->
# 생성형 AI를 책임감 있게 사용하기

[![생성형 AI를 책임감 있게 사용하기](../../../03-using-generative-ai-responsibly/images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://youtu.be/YOp-e1GjZdA?si=7Wv4wu3x44L1DCVj)

> _(위 이미지를 클릭하면 이 강의의 동영상을 볼 수 있습니다)_

AI와 특히 생성형 AI에 매료되기는 쉽지만, 어떻게 책임감 있게 사용할 것인지 고려해야 합니다. 출력이 공정하고 해롭지 않은지 확인하는 방법 등을 생각해야 합니다. 이 장은 언급된 맥락, 고려해야 할 사항, AI 사용을 개선하기 위한 적극적인 조치를 취하는 방법을 제공하는 것을 목표로 합니다.

## 소개

이 강의에서는 다음 내용을 다룹니다:

- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI를 우선시해야 하는 이유.
- 책임감 있는 AI의 핵심 원칙과 생성형 AI와의 관계.
- 전략과 도구를 통해 이러한 책임감 있는 AI 원칙을 실천하는 방법.

## 학습 목표

이 강의를 완료한 후, 여러분은 다음을 알게 됩니다:

- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI의 중요성.
- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI의 핵심 원칙을 언제 생각하고 적용해야 하는지.
- 책임감 있는 AI 개념을 실천하기 위해 사용할 수 있는 도구와 전략.

## 책임감 있는 AI 원칙

생성형 AI에 대한 기대는 그 어느 때보다 높습니다. 이러한 기대는 이 분야에 많은 새로운 개발자, 관심, 자금을 가져왔습니다. 생성형 AI를 사용하여 제품과 회사를 구축하려는 모든 사람에게 이것은 매우 긍정적이지만, 책임감 있게 진행하는 것도 중요합니다.

이 과정 전반에 걸쳐 우리는 스타트업과 AI 교육 제품을 구축하는 데 중점을 둡니다. 우리는 책임감 있는 AI의 원칙인 공정성, 포괄성, 신뢰성/안전성, 보안 및 개인정보 보호, 투명성 및 책임성을 사용할 것입니다. 이러한 원칙을 통해 제품에서 생성형 AI를 사용하는 것과 어떻게 관련되는지 살펴보겠습니다.

## 왜 책임감 있는 AI를 우선시해야 할까요

제품을 구축할 때 사용자의 최선의 이익을 염두에 두고 인간 중심적 접근 방식을 취하면 최상의 결과를 얻을 수 있습니다.

생성형 AI의 독특함은 사용자에게 유용한 답변, 정보, 지침 및 콘텐츠를 생성할 수 있는 능력입니다. 이는 많은 수동 단계 없이 수행할 수 있어 매우 인상적인 결과를 낳을 수 있습니다. 적절한 계획과 전략 없이는 불행히도 사용자, 제품 및 사회 전체에 해로운 결과를 초래할 수도 있습니다.

잠재적으로 해로운 결과의 일부(전부는 아님)를 살펴보겠습니다:

### 환각(Hallucinations)

환각은 LLM이 완전히 말도 안 되거나 다른 정보 출처를 기반으로 사실적으로 틀렸다고 알고 있는 콘텐츠를 생성할 때 사용되는 용어입니다.

예를 들어 우리 스타트업에서 학생들이 모델에 역사적 질문을 할 수 있는 기능을 구축한다고 가정해 봅시다. 학생이 `타이타닉호의 유일한 생존자는 누구였나요?`라는 질문을 합니다.

모델은 아래와 같은 응답을 생성합니다:

![타이타닉호의 유일한 생존자는 누구였나요라는 프롬프트](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> _(출처: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

이것은 매우 자신감 있고 철저한 답변입니다. 불행히도 이것은 틀렸습니다. 최소한의 조사만으로도 타이타닉 재난에서 한 명 이상의 생존자가 있었다는 것을 발견할 수 있습니다. 이 주제를 막 연구하기 시작한 학생에게 이 답변은 의심받지 않고 사실로 받아들여질 만큼 설득력이 있을 수 있습니다. 이것의 결과는 AI 시스템이 신뢰할 수 없게 되고 스타트업의 평판에 부정적인 영향을 미칠 수 있습니다.

주어진 LLM의 각 반복마다 환각을 최소화하는 것과 관련하여 성능 향상을 보았습니다. 이러한 개선에도 불구하고 애플리케이션 빌더와 사용자로서 우리는 여전히 이러한 제한 사항을 인식해야 합니다.

### 유해한 콘텐츠

이전 섹션에서 LLM이 잘못되거나 말도 안 되는 응답을 생성할 때를 다루었습니다. 우리가 인식해야 할 또 다른 위험은 모델이 유해한 콘텐츠로 응답할 때입니다.

유해한 콘텐츠는 다음과 같이 정의될 수 있습니다:

- 자해 또는 특정 그룹에 대한 해를 끼치는 지침 제공 또는 장려.
- 증오적이거나 비하하는 콘텐츠.
- 모든 유형의 공격이나 폭력 행위 계획 안내.
- 불법 콘텐츠를 찾거나 불법 행위를 저지르는 방법에 대한 지침 제공.
- 성적으로 노골적인 콘텐츠 표시.

우리 스타트업의 경우 학생들이 이러한 유형의 콘텐츠를 보지 못하도록 적절한 도구와 전략을 갖추고 있는지 확인하고 싶습니다.

### 공정성 부족

공정성은 "AI 시스템이 편견과 차별로부터 자유롭고 모든 사람을 공정하고 평등하게 대우하도록 보장하는 것"으로 정의됩니다. 생성형 AI의 세계에서 우리는 소외된 그룹의 배타적인 세계관이 모델의 출력에 의해 강화되지 않도록 보장하고자 합니다.

이러한 유형의 출력은 사용자를 위한 긍정적인 제품 경험을 구축하는 데 파괴적일 뿐만 아니라 사회에 더 큰 해를 끼칩니다. 애플리케이션 빌더로서 우리는 생성형 AI로 솔루션을 구축할 때 항상 광범위하고 다양한 사용자 기반을 염두에 두어야 합니다.

## 생성형 AI를 책임감 있게 사용하는 방법

이제 책임감 있는 생성형 AI의 중요성을 확인했으므로 AI 솔루션을 책임감 있게 구축하기 위해 취할 수 있는 4단계를 살펴보겠습니다:

![완화 사이클](../../../03-using-generative-ai-responsibly/images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### 잠재적 피해 측정

소프트웨어 테스트에서 우리는 애플리케이션에서 사용자의 예상 작업을 테스트합니다. 마찬가지로 사용자가 사용할 가능성이 가장 높은 다양한 프롬프트 세트를 테스트하는 것은 잠재적 피해를 측정하는 좋은 방법입니다.

우리 스타트업은 교육 제품을 구축하고 있으므로 교육 관련 프롬프트 목록을 준비하는 것이 좋습니다. 이것은 특정 과목, 역사적 사실 및 학생 생활에 대한 프롬프트를 다루는 것일 수 있습니다.

### 잠재적 피해 완화

이제 모델과 그 응답으로 인한 잠재적 피해를 방지하거나 제한할 수 있는 방법을 찾을 때입니다. 우리는 이것을 4가지 다른 계층에서 볼 수 있습니다:

![완화 계층](../../../03-using-generative-ai-responsibly/images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **모델**. 올바른 사용 사례에 올바른 모델 선택. GPT-4와 같은 더 크고 복잡한 모델은 더 작고 구체적인 사용 사례에 적용될 때 유해한 콘텐츠의 위험을 더 많이 야기할 수 있습니다. 훈련 데이터를 사용하여 미세 조정하면 유해한 콘텐츠의 위험도 줄어듭니다.

- **안전 시스템**. 안전 시스템은 모델을 제공하는 플랫폼의 피해를 완화하는 데 도움이 되는 도구 및 구성 세트입니다. 이것의 예로는 Azure OpenAI 서비스의 콘텐츠 필터링 시스템이 있습니다. 시스템은 또한 탈옥 공격과 봇의 요청과 같은 원치 않는 활동을 감지해야 합니다.

- **메타프롬프트**. 메타프롬프트와 그라운딩은 특정 동작과 정보를 기반으로 모델을 지시하거나 제한할 수 있는 방법입니다. 이것은 시스템 입력을 사용하여 모델의 특정 제한을 정의하는 것일 수 있습니다. 또한 시스템의 범위 또는 도메인과 더 관련된 출력을 제공합니다.

또한 RAG(검색 증강 생성)와 같은 기술을 사용하여 모델이 신뢰할 수 있는 출처의 선택에서만 정보를 가져오도록 할 수도 있습니다. 이 과정의 나중에 [검색 애플리케이션 구축](../../../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)에 대한 강의가 있습니다.

- **사용자 경험**. 최종 계층은 사용자가 애플리케이션의 인터페이스를 통해 어떤 방식으로든 모델과 직접 상호 작용하는 곳입니다. 이런 방식으로 우리는 사용자가 모델에 보낼 수 있는 입력 유형과 사용자에게 표시되는 텍스트 또는 이미지를 제한하도록 UI/UX를 설계할 수 있습니다. AI 애플리케이션을 배포할 때 우리는 또한 생성형 AI 애플리케이션이 할 수 있는 것과 할 수 없는 것에 대해 투명해야 합니다.

[AI 애플리케이션을 위한 UX 디자인](../../../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)에 전념하는 전체 강의가 있습니다.

- **모델 평가**. LLM 작업은 모델이 훈련된 데이터를 항상 제어할 수 없기 때문에 어려울 수 있습니다. 그럼에도 불구하고 우리는 항상 모델의 성능과 출력을 평가해야 합니다. 모델의 정확성, 유사성, 근거 및 출력의 관련성을 측정하는 것은 여전히 중요합니다. 이것은 이해 관계자와 사용자에게 투명성과 신뢰를 제공하는 데 도움이 됩니다.

### 책임감 있는 생성형 AI 솔루션 운영

AI 애플리케이션을 중심으로 운영 관행을 구축하는 것이 최종 단계입니다. 여기에는 모든 규제 정책을 준수하도록 법무 및 보안과 같은 스타트업의 다른 부분과 파트너십을 맺는 것이 포함됩니다. 출시 전에 우리는 또한 사용자에게 피해가 커지는 것을 방지하기 위해 전달, 사고 처리 및 롤백에 대한 계획을 세우고 싶습니다.

## 도구

책임감 있는 AI 솔루션을 개발하는 작업은 많은 것처럼 보일 수 있지만 노력할 가치가 있는 작업입니다. 생성형 AI 영역이 성장함에 따라 개발자가 책임을 워크플로에 효율적으로 통합하는 데 도움이 되는 더 많은 도구가 성숙해질 것입니다. 예를 들어 [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)는 API 요청을 통해 유해한 콘텐츠와 이미지를 감지하는 데 도움이 될 수 있습니다.

## 지식 점검

책임감 있는 AI 사용을 보장하기 위해 주의해야 할 사항은 무엇인가요?

1. 답변이 정확한지 확인.
1. 유해한 사용, AI가 범죄 목적으로 사용되지 않도록 확인.
1. AI가 편견과 차별로부터 자유롭도록 보장.

A: 2와 3이 정답입니다. 책임감 있는 AI는 유해한 영향과 편견 등을 완화하는 방법을 고려하는 데 도움이 됩니다.

## 🚀 도전 과제

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)에 대해 읽어보고 사용에 채택할 수 있는 것을 확인하세요.

## 훌륭한 작업! 학습을 계속하세요

이 강의를 완료한 후 [생성형 AI 학습 컬렉션](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 생성형 AI 지식을 계속 향상시키세요!

Lesson 4로 이동하여 [프롬프트 엔지니어링 기초](../../../04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)를 살펴보세요!